{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/26 10:45:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence = (\n",
    "    spark.read.parquet(\"../../evidence\")\n",
    "    .filter(f.col(\"variantId\").isNotNull())\n",
    "    .withColumn('chr', f.split(f.col('variantId'), '_').getItem(0))\n",
    "    .withColumn('genomicLocation', f.split(f.col('variantId'), '_').getItem(1))\n",
    "    .groupBy('chr', 'genomicLocation')\n",
    "    .agg(\n",
    "        f.collect_set(f.struct(f.col('variantId'), f.col('diseaseId'), f.col('diseaseFromSource'))).alias('evidenceInfo')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "756504"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:====================================================>  (153 + 6) / 159]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|chr|genomicLocation|evidenceInfo                                                                                                                                                                                                                       |\n",
      "+---+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |100188948      |[{1_100188948_C_G, Orphanet_511, Maple syrup urine disease}]                                                                                                                                                                       |\n",
      "|1  |100192396      |[{1_100192396_A_G, Orphanet_511, Maple syrup urine disease}]                                                                                                                                                                       |\n",
      "|1  |100195337      |[{1_100195337_C_A, Orphanet_511, Maple syrup urine disease}]                                                                                                                                                                       |\n",
      "|1  |100206452      |[{1_100206452_A_G, Orphanet_511, Maple syrup urine disease}]                                                                                                                                                                       |\n",
      "|1  |100214903      |[{1_100214903_T_G, Orphanet_511, Maple syrup urine disease}]                                                                                                                                                                       |\n",
      "|1  |100218752      |[{1_100218752_T_G, Orphanet_511, Maple syrup urine disease}]                                                                                                                                                                       |\n",
      "|1  |100539839      |[{1_100539839_C_A, MONDO_0014839, Chorea, childhood-onset, with psychomotor retardation}]                                                                                                                                          |\n",
      "|1  |1010747        |[{1_1010747_GT_G, EFO_0007010, Prozac 20mg capsule | treatment/medication code}]                                                                                                                                                   |\n",
      "|1  |101178166      |[{1_101178166_C_T, EFO_0004842, Eosinophill count}, {1_101178166_C_T, EFO_0007991, Eosinophill percentage}]                                                                                                                        |\n",
      "|1  |1014143        |[{1_1014143_C_T, Orphanet_319563, Immunodeficiency 38 with basal ganglia calcification}]                                                                                                                                           |\n",
      "|1  |10256234       |[{1_10256234_T_C, Orphanet_166, Charcot-Marie-Tooth disease}]                                                                                                                                                                      |\n",
      "|1  |102879759      |[{1_102879759_C_T, EFO_1001986, Connective tissue disease}, {1_102879759_C_T, Orphanet_560, Marshall syndrome}, {1_102879759_C_T, Orphanet_828, Stickler syndrome type 2}, {1_102879759_C_T, Orphanet_2021, Fibrochondrogenesis 1}]|\n",
      "|1  |102913693      |[{1_102913693_A_G, Orphanet_828, Stickler syndrome type 2}, {1_102913693_A_G, Orphanet_2021, Fibrochondrogenesis 1}]                                                                                                               |\n",
      "|1  |102935110      |[{1_102935110_G_C, Orphanet_2021, Fibrochondrogenesis 1}]                                                                                                                                                                          |\n",
      "|1  |103105081      |[{1_103105081_A_AT, EFO_0007788, Waist-to-hip ratio adjusted for BMI}]                                                                                                                                                             |\n",
      "|1  |103122951      |[{1_103122951_A_G, EFO_0008354, Cognitive performance (MTAG) [MTAG]}]                                                                                                                                                              |\n",
      "|1  |10342040       |[{1_10342040_T_A, Orphanet_166, Charcot-Marie-Tooth disease, type 2}]                                                                                                                                                              |\n",
      "|1  |10345896       |[{1_10345896_T_C, EFO_0000621, Neuroblastoma}]                                                                                                                                                                                     |\n",
      "|1  |10361040       |[{1_10361040_A_T, Orphanet_166, Charcot-Marie-Tooth disease, type 2}]                                                                                                                                                              |\n",
      "|1  |10365440       |[{1_10365440_G_A, EFO_0000621, Neuroblastoma}, {1_10365440_G_A, Orphanet_166, Charcot-Marie-Tooth disease, type 2}]                                                                                                                |\n",
      "+---+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evidence.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gen_location = (\n",
    "    spark.read.json(\"residue_gen_pos_output/residue_genomic_position.json\")\n",
    ")\n",
    "\n",
    "gen_location = (\n",
    "    gen_location\n",
    "    .withColumn(\"pdbCompound\", gen_location[\"resInfos.compound\"])\n",
    "    .withColumn(\"resNb\", gen_location[\"resInfos.res_nb\"])\n",
    "    .withColumn(\"chain\", gen_location[\"resInfos.chain\"])\n",
    "    .withColumn(\"resType\", gen_location[\"resInfos.res_type\"])\n",
    "    .withColumn(\"interType\", gen_location[\"resInfos.inter_type\"])\n",
    "    .withColumn(\"chr\", gen_location[\"resInfos.chromosome\"])\n",
    "    .withColumn(\"genLocation_1\", gen_location[\"resInfos.genLocation.res_pos_1\"])\n",
    "    .withColumn(\"genLocation_2\", gen_location[\"resInfos.genLocation.res_pos_2\"])\n",
    "    .withColumn(\"genLocation_3\", gen_location[\"resInfos.genLocation.res_pos_3\"])\n",
    "    .drop(\"resInfos\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivot_expression = '''stack(3, 'genLocation_1', genLocation_1, 'genLocation_2', genLocation_2, 'genLocation_3', genLocation_3) as (genLocation_label, genLocation_val)'''\n",
    "\n",
    "gen_location_unpivot = (\n",
    "    gen_location\n",
    "        .select('geneId', 'pdbStructId', 'pdbCompound', 'resNb', 'chain', 'resType', 'chr', f.expr(unpivot_expression))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "788565"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_location_unpivot.count()  # More lines but less operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----+-----+-------+---+-----------------+---------------+\n",
      "|         geneId|pdbStructId|pdbCompound|resNb|chain|resType|chr|genLocation_label|genLocation_val|\n",
      "+---------------+-----------+-----------+-----+-----+-------+---+-----------------+---------------+\n",
      "|ENSG00000001626|       1xmj|        ATP|  466|    A|    SER|  7|    genLocation_1|      117559467|\n",
      "|ENSG00000001626|       1xmj|        ATP|  466|    A|    SER|  7|    genLocation_2|      117559468|\n",
      "|ENSG00000001626|       1xmj|        ATP|  466|    A|    SER|  7|    genLocation_3|      117559469|\n",
      "|ENSG00000001626|       1xmj|        ATP|  465|    A|    THR|  7|    genLocation_1|      117559464|\n",
      "|ENSG00000001626|       1xmj|        ATP|  465|    A|    THR|  7|    genLocation_2|      117559465|\n",
      "|ENSG00000001626|       1xmj|        ATP|  465|    A|    THR|  7|    genLocation_3|      117559466|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_1|      117548821|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_2|      117548822|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_3|      117548823|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_1|      117548641|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_2|      117548642|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_3|      117548643|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_1|      117542106|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_2|      117542107|\n",
      "|ENSG00000001626|       1xmj|        ATP|  464|    A|    LYS|  7|    genLocation_3|      117542108|\n",
      "|ENSG00000001626|       1xmj|        ATP|  460|    A|    THR|  7|    genLocation_1|      117548809|\n",
      "|ENSG00000001626|       1xmj|        ATP|  460|    A|    THR|  7|    genLocation_2|      117548810|\n",
      "|ENSG00000001626|       1xmj|        ATP|  460|    A|    THR|  7|    genLocation_3|      117548811|\n",
      "|ENSG00000001626|       1xmj|        ATP|  461|    A|    GLY|  7|    genLocation_1|      117548812|\n",
      "|ENSG00000001626|       1xmj|        ATP|  461|    A|    GLY|  7|    genLocation_2|      117548813|\n",
      "+---------------+-----------+-----------+-----+-----+-------+---+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gen_location_unpivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_with_disease = (\n",
    "    gen_location_unpivot.join(\n",
    "        evidence, \n",
    "        (gen_location_unpivot.chr == evidence.chr) &\n",
    "        (gen_location_unpivot.genLocation_val == evidence.genomicLocation)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12728"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_with_disease.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/26 10:46:09 ERROR Executor: Exception in task 39.0 in stage 13.0 (TID 413)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/04/26 10:46:09 ERROR Executor: Exception in task 104.0 in stage 13.0 (TID 478)\n",
      "java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 16392 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader, or disable spark.sql.sources.bucketing.enabled if you read from bucket table. For Parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for ORC file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:113)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:93)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:488)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:507)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.putByteArray(WritableColumnVector.java:358)\n",
      "\tat org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:343)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:88)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:576)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:91)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:488)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:507)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.putByteArray(WritableColumnVector.java:358)\n",
      "\tat org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:343)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$Lambda$2932/0x000000084121f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "22/04/26 10:46:09 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 39.0 in stage 13.0 (TID 413),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/04/26 10:46:09 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 104.0 in stage 13.0 (TID 478),5,main]\n",
      "java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 16392 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader, or disable spark.sql.sources.bucketing.enabled if you read from bucket table. For Parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for ORC file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:113)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:93)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:488)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:507)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.putByteArray(WritableColumnVector.java:358)\n",
      "\tat org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:343)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:88)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:576)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:91)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:488)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:507)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.putByteArray(WritableColumnVector.java:358)\n",
      "\tat org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:343)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$Lambda$2932/0x000000084121f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "22/04/26 10:46:10 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5e9a386c rejected from java.util.concurrent.ThreadPoolExecutor@75e65b8f[Shutting down, pool size = 62, active threads = 62, queued tasks = 0, completed tasks = 458]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@690267b7 rejected from java.util.concurrent.ThreadPoolExecutor@75e65b8f[Shutting down, pool size = 62, active threads = 62, queued tasks = 0, completed tasks = 458]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6c736d0f rejected from java.util.concurrent.ThreadPoolExecutor@75e65b8f[Shutting down, pool size = 61, active threads = 61, queued tasks = 0, completed tasks = 459]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7ead1c7d rejected from java.util.concurrent.ThreadPoolExecutor@75e65b8f[Shutting down, pool size = 61, active threads = 61, queued tasks = 0, completed tasks = 459]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 WARN TaskSetManager: Lost task 39.0 in stage 13.0 (TID 413) (mg-internship-plip.c.open-targets-eu-dev.internal executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "22/04/26 10:46:10 ERROR TaskSetManager: Task 39 in stage 13.0 failed 1 times; aborting job\n",
      "22/04/26 10:46:10 WARN TaskSetManager: Lost task 104.0 in stage 13.0 (TID 478) (mg-internship-plip.c.open-targets-eu-dev.internal executor driver): java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 16392 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader, or disable spark.sql.sources.bucketing.enabled if you read from bucket table. For Parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for ORC file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:113)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:93)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:488)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:507)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.putByteArray(WritableColumnVector.java:358)\n",
      "\tat org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:343)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:522)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:88)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:576)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:91)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:488)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:507)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.putByteArray(WritableColumnVector.java:358)\n",
      "\tat org.apache.spark.sql.execution.vectorized.ColumnVectorUtils.populate(ColumnVectorUtils.java:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.initBatch(VectorizedParquetRecordReader.java:264)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:343)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$Lambda$2932/0x000000084121f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)\n",
      "\n",
      "22/04/26 10:46:10 ERROR TaskSchedulerImpl: Exception in statusUpdate+ 65) / 159]\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$5405/0x00000008412e5840@7a3f65e5 rejected from java.util.concurrent.ThreadPoolExecutor@61ab2fea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 461]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:817)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:791)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/27/temp_shuffle_22a2f020-fee0-407e-a4c1-8a66c8613d28\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/27/temp_shuffle_22a2f020-fee0-407e-a4c1-8a66c8613d28 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/27/temp_shuffle_22a2f020-fee0-407e-a4c1-8a66c8613d28\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_88219b88-98e5-416e-8728-058b6275557c\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_88219b88-98e5-416e-8728-058b6275557c (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_88219b88-98e5-416e-8728-058b6275557c\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3c/temp_shuffle_c8b1ba3e-e2d8-4740-86bc-369252f69d8c\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3c/temp_shuffle_c8b1ba3e-e2d8-4740-86bc-369252f69d8c (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3c/temp_shuffle_c8b1ba3e-e2d8-4740-86bc-369252f69d8c\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2d/temp_shuffle_3f312d2c-c77e-4c2d-b8bb-308214b47880\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2d/temp_shuffle_3f312d2c-c77e-4c2d-b8bb-308214b47880 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2d/temp_shuffle_3f312d2c-c77e-4c2d-b8bb-308214b47880\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2e/temp_shuffle_d162314f-c944-4a46-9fbc-5b17b8095518\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2e/temp_shuffle_d162314f-c944-4a46-9fbc-5b17b8095518 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2e/temp_shuffle_d162314f-c944-4a46-9fbc-5b17b8095518\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/33/temp_shuffle_cc6c8bfc-6d09-482b-888a-91f4130f0e18\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/33/temp_shuffle_cc6c8bfc-6d09-482b-888a-91f4130f0e18 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/33/temp_shuffle_cc6c8bfc-6d09-482b-888a-91f4130f0e18\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/04/temp_shuffle_94de1da8-999a-40e9-a8f1-76b2d522409d\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/04/temp_shuffle_94de1da8-999a-40e9-a8f1-76b2d522409d (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/04/temp_shuffle_94de1da8-999a-40e9-a8f1-76b2d522409d\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/21/temp_shuffle_14f274bc-7998-4a3f-87d1-338f4c83682f\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/21/temp_shuffle_14f274bc-7998-4a3f-87d1-338f4c83682f (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/21/temp_shuffle_14f274bc-7998-4a3f-87d1-338f4c83682f\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2d/temp_shuffle_b4db809d-f0e1-44d1-b8ba-1c266cc60c42\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2d/temp_shuffle_b4db809d-f0e1-44d1-b8ba-1c266cc60c42 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2d/temp_shuffle_b4db809d-f0e1-44d1-b8ba-1c266cc60c42\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0f/temp_shuffle_9109a878-6f69-4378-ae19-29ce297bd063\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0f/temp_shuffle_9109a878-6f69-4378-ae19-29ce297bd063 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0f/temp_shuffle_9109a878-6f69-4378-ae19-29ce297bd063\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1a/temp_shuffle_b06300fd-d846-4f95-90a4-fe4b233a31bf\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1a/temp_shuffle_b06300fd-d846-4f95-90a4-fe4b233a31bf (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1a/temp_shuffle_b06300fd-d846-4f95-90a4-fe4b233a31bf\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3d/temp_shuffle_c5cb83ac-d7a7-4da2-a8e7-a53886d7ff16\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3d/temp_shuffle_c5cb83ac-d7a7-4da2-a8e7-a53886d7ff16 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3d/temp_shuffle_c5cb83ac-d7a7-4da2-a8e7-a53886d7ff16\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/05/temp_shuffle_4bb6dbe2-6171-4752-bb49-04d714c5feb8\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/05/temp_shuffle_4bb6dbe2-6171-4752-bb49-04d714c5feb8 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/05/temp_shuffle_4bb6dbe2-6171-4752-bb49-04d714c5feb8\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/16/temp_shuffle_01be4d81-dd52-46cf-923e-3356140f65c9\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/16/temp_shuffle_01be4d81-dd52-46cf-923e-3356140f65c9 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/16/temp_shuffle_01be4d81-dd52-46cf-923e-3356140f65c9\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/01/temp_shuffle_5a51145d-5652-462f-ae64-2870e1348192\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/01/temp_shuffle_5a51145d-5652-462f-ae64-2870e1348192 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/01/temp_shuffle_5a51145d-5652-462f-ae64-2870e1348192\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/34/temp_shuffle_458aee51-4e75-4403-a98a-5038d8355efb\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/34/temp_shuffle_458aee51-4e75-4403-a98a-5038d8355efb (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/34/temp_shuffle_458aee51-4e75-4403-a98a-5038d8355efb\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/24/temp_shuffle_7f96d7e0-ae41-4eb6-97d2-93d3499252c0\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/24/temp_shuffle_7f96d7e0-ae41-4eb6-97d2-93d3499252c0 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/24/temp_shuffle_7f96d7e0-ae41-4eb6-97d2-93d3499252c0\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/25/temp_shuffle_b48baf88-daa9-430c-9870-46e90e602c80\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/25/temp_shuffle_b48baf88-daa9-430c-9870-46e90e602c80 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/25/temp_shuffle_b48baf88-daa9-430c-9870-46e90e602c80\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/20/temp_shuffle_4427b120-b059-4346-b3c2-99076cbda856\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/20/temp_shuffle_4427b120-b059-4346-b3c2-99076cbda856 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/20/temp_shuffle_4427b120-b059-4346-b3c2-99076cbda856\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/20/temp_shuffle_78cb053f-3497-43e5-aa80-67eee55f5db0\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/20/temp_shuffle_78cb053f-3497-43e5-aa80-67eee55f5db0 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/20/temp_shuffle_78cb053f-3497-43e5-aa80-67eee55f5db0\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2f/temp_shuffle_2ae76a58-854e-47ca-b132-2e7fce60799e\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2f/temp_shuffle_2ae76a58-854e-47ca-b132-2e7fce60799e (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/2f/temp_shuffle_2ae76a58-854e-47ca-b132-2e7fce60799e\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0a/temp_shuffle_db5753fe-a673-44f9-8bb5-cd02e84123cf\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0a/temp_shuffle_db5753fe-a673-44f9-8bb5-cd02e84123cf (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0a/temp_shuffle_db5753fe-a673-44f9-8bb5-cd02e84123cf\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/03/temp_shuffle_b4f88f5d-ec7a-4e68-9048-f3872b81049a\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/03/temp_shuffle_b4f88f5d-ec7a-4e68-9048-f3872b81049a (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/03/temp_shuffle_b4f88f5d-ec7a-4e68-9048-f3872b81049a\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1d/temp_shuffle_7c2a8fd3-9431-49a9-8ffe-f75cc71d8e6f\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1d/temp_shuffle_7c2a8fd3-9431-49a9-8ffe-f75cc71d8e6f (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1d/temp_shuffle_7c2a8fd3-9431-49a9-8ffe-f75cc71d8e6f\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0e/temp_shuffle_769606b4-220f-4cf0-b974-16605f74f656\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0e/temp_shuffle_769606b4-220f-4cf0-b974-16605f74f656 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0e/temp_shuffle_769606b4-220f-4cf0-b974-16605f74f656\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/06/temp_shuffle_f80b116b-5066-4144-ad1d-6f7c04405ff2\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/06/temp_shuffle_f80b116b-5066-4144-ad1d-6f7c04405ff2 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/06/temp_shuffle_f80b116b-5066-4144-ad1d-6f7c04405ff2\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_d7a04c6d-1ef8-4870-bbc8-208673676287\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_d7a04c6d-1ef8-4870-bbc8-208673676287 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_d7a04c6d-1ef8-4870-bbc8-208673676287\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/38/temp_shuffle_5f1134f7-f611-401c-a8db-cd1b2202b4c8\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/38/temp_shuffle_5f1134f7-f611-401c-a8db-cd1b2202b4c8 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/38/temp_shuffle_5f1134f7-f611-401c-a8db-cd1b2202b4c8\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/31/temp_shuffle_0d0dfb3d-1676-49da-aada-d7b8010d7b67\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/31/temp_shuffle_0d0dfb3d-1676-49da-aada-d7b8010d7b67 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/31/temp_shuffle_0d0dfb3d-1676-49da-aada-d7b8010d7b67\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1f/temp_shuffle_c1757e19-efdc-41dd-8750-867ab78bb0a5\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1f/temp_shuffle_c1757e19-efdc-41dd-8750-867ab78bb0a5 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1f/temp_shuffle_c1757e19-efdc-41dd-8750-867ab78bb0a5\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/35/temp_shuffle_e56941b7-e086-42bf-8b9e-be637ef135ab\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/35/temp_shuffle_e56941b7-e086-42bf-8b9e-be637ef135ab (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/35/temp_shuffle_e56941b7-e086-42bf-8b9e-be637ef135ab\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0d/temp_shuffle_c7267102-6151-469b-917d-4f4b28d359dd\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0d/temp_shuffle_c7267102-6151-469b-917d-4f4b28d359dd (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0d/temp_shuffle_c7267102-6151-469b-917d-4f4b28d359dd\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/05/temp_shuffle_6ffc9dc9-aa05-4aca-9ddd-9970a1177db1\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/05/temp_shuffle_6ffc9dc9-aa05-4aca-9ddd-9970a1177db1 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/05/temp_shuffle_6ffc9dc9-aa05-4aca-9ddd-9970a1177db1\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/23/temp_shuffle_bb864a1a-4d28-48a8-9c05-c711621d9f5f\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/23/temp_shuffle_bb864a1a-4d28-48a8-9c05-c711621d9f5f (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/23/temp_shuffle_bb864a1a-4d28-48a8-9c05-c711621d9f5f\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/22/temp_shuffle_5ab753ae-5cf1-43d0-9f67-520c9a353b74\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/22/temp_shuffle_5ab753ae-5cf1-43d0-9f67-520c9a353b74 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/22/temp_shuffle_5ab753ae-5cf1-43d0-9f67-520c9a353b74\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/08/temp_shuffle_00b64be9-13c6-40c1-9178-03c8a6c1e27d\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/08/temp_shuffle_00b64be9-13c6-40c1-9178-03c8a6c1e27d (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/08/temp_shuffle_00b64be9-13c6-40c1-9178-03c8a6c1e27d\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/39/temp_shuffle_9fc6fc1f-cca8-4258-980d-b580992fdcd3\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/39/temp_shuffle_9fc6fc1f-cca8-4258-980d-b580992fdcd3 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/39/temp_shuffle_9fc6fc1f-cca8-4258-980d-b580992fdcd3\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0c/temp_shuffle_989536bc-1a5b-4694-8324-5879b6b3333b\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0c/temp_shuffle_989536bc-1a5b-4694-8324-5879b6b3333b (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/0c/temp_shuffle_989536bc-1a5b-4694-8324-5879b6b3333b\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/21/temp_shuffle_8693ea2d-17d3-4b08-8cb0-4f00a4f098f8\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/21/temp_shuffle_8693ea2d-17d3-4b08-8cb0-4f00a4f098f8 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/21/temp_shuffle_8693ea2d-17d3-4b08-8cb0-4f00a4f098f8\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1c/temp_shuffle_4e1a7a81-1005-4a93-a0b2-7b1807f8fa05\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1c/temp_shuffle_4e1a7a81-1005-4a93-a0b2-7b1807f8fa05 (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/1c/temp_shuffle_4e1a7a81-1005-4a93-a0b2-7b1807f8fa05\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3c/temp_shuffle_82f1d73f-f401-4139-bce0-635baee6deee\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3c/temp_shuffle_82f1d73f-f401-4139-bce0-635baee6deee (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3c/temp_shuffle_82f1d73f-f401-4139-bce0-635baee6deee\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_d9973d4b-d931-4474-8abb-4261fc4be31a\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_d9973d4b-d931-4474-8abb-4261fc4be31a (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/26 10:46:10 ERROR BypassMergeSortShuffleWriter: Error while deleting file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/26/temp_shuffle_d9973d4b-d931-4474-8abb-4261fc4be31a\n",
      "22/04/26 10:46:10 ERROR DiskBlockObjectWriter: Uncaught exception while reverting partial writes to file /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3f/temp_shuffle_9e73e4b4-7943-4804-b440-51448da3560c\n",
      "java.io.FileNotFoundException: /tmp/blockmgr-348be179-51f6-47e5-8be7-89470a5f90ae/3f/temp_shuffle_9e73e4b4-7943-4804-b440-51448da3560c (No such file or directory)\n",
      "\tat java.base/java.io.FileOutputStream.open0(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:298)\n",
      "\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:237)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.$anonfun$revertPartialWritesAndClose$2(DiskBlockObjectWriter.scala:253)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1471)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.revertPartialWritesAndClose(DiskBlockObjectWriter.scala:250)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.stop(BypassMergeSortShuffleWriter.java:292)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:81)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_11815/2640162406.py\", line 1, in <cell line: 1>\n",
      "    res_with_disease.show()\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 494, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m/home/marinegirardey/OpenTargetInternship/scripts/diseases_association.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmg-internship-plip.europe-west1-d.open-targets-eu-dev/home/marinegirardey/OpenTargetInternship/scripts/diseases_association.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m res_with_disease\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=492'>493</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=493'>494</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=494'>495</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:1993\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py?line=1989'>1990</a>\u001b[0m     traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py?line=1990'>1991</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py?line=1992'>1993</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_showtraceback(etype, value, stb)\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py?line=1993'>1994</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_pdb:\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py?line=1994'>1995</a>\u001b[0m     \u001b[39m# drop into debugger\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py?line=1995'>1996</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebugger(force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py:538\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=531'>532</a>\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=532'>533</a>\u001b[0m sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=534'>535</a>\u001b[0m exc_content \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=535'>536</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtraceback\u001b[39m\u001b[39m\"\u001b[39m: stb,\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=536'>537</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mename\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(etype\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m),\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=537'>538</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevalue\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39;49m(evalue),\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=538'>539</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=540'>541</a>\u001b[0m dh \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayhook\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=541'>542</a>\u001b[0m \u001b[39m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/ipykernel/zmqshell.py?line=542'>543</a>\u001b[0m \u001b[39m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=468'>469</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=469'>470</a>\u001b[0m     gateway_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_exception\u001b[39m.\u001b[39m_gateway_client\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=470'>471</a>\u001b[0m     answer \u001b[39m=\u001b[39m gateway_client\u001b[39m.\u001b[39;49msend_command(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexception_cmd)\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=471'>472</a>\u001b[0m     return_value \u001b[39m=\u001b[39m get_return_value(answer, gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=472'>473</a>\u001b[0m     \u001b[39m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=473'>474</a>\u001b[0m     \u001b[39m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/protocol.py?line=474'>475</a>\u001b[0m     \u001b[39m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1014'>1015</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1015'>1016</a>\u001b[0m     \u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1016'>1017</a>\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1017'>1018</a>\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1033'>1034</a>\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1034'>1035</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1035'>1036</a>\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1036'>1037</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/java_gateway.py?line=1037'>1038</a>\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py:281\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=277'>278</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=279'>280</a>\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=280'>281</a>\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=281'>282</a>\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py:288\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=283'>284</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=284'>285</a>\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=285'>286</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=286'>287</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=287'>288</a>\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=288'>289</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=289'>290</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py:402\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=398'>399</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=399'>400</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=400'>401</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=401'>402</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=402'>403</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/marinegirardey/OpenTargetInternship/venv/lib/python3.8/site-packages/py4j/clientserver.py?line=403'>404</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "res_with_disease.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_with_disease_cleaned = (\n",
    "\n",
    "        res_with_disease\n",
    "\n",
    "        .drop(\"genLocation_1\", \"genLocation_2\", \"genLocation_3\", \"targetId\", \"chr\", \"genomicLocation\")\n",
    "\n",
    "        .groupby([f.col('geneId'),\n",
    "                f.col('pdbStructId'),\n",
    "                f.col(\"resNb\"),\n",
    "                f.col(\"resType\"),\n",
    "                f.col(\"interType\"),\n",
    "                f.col(\"diseaseId\"),\n",
    "                f.col(\"diseaseFromSource\")\n",
    "                ])\n",
    "\n",
    "        .agg(\n",
    "                f.collect_set(f.struct(f.col('variantId'))).alias(\"variantIds\"),\n",
    "                f.collect_set(f.col('pdbStructId')).alias(\"pdbStructIds\"),\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----+-------+-------------+-------------+--------------------+--------------------+------------+\n",
      "|         geneId|pdbStructId|resNb|resType|    interType|    diseaseId|   diseaseFromSource|          variantIds|pdbStructIds|\n",
      "+---------------+-----------+-----+-------+-------------+-------------+--------------------+--------------------+------------+\n",
      "|ENSG00000001626|       1xmi|  401|    TRP|      pistack| Orphanet_586|     Cystic fibrosis|[{7_117542101_G_A...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  462|    ALA|        hbond| Orphanet_586|     Cystic fibrosis| [{7_117548815_G_A}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  464|    LYS|        hbond| Orphanet_586|     Cystic fibrosis|[{7_117542108_G_C...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  464|    LYS|        hbond| Orphanet_676|Hereditary pancre...| [{7_117548823_G_T}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  464|    LYS|   saltbridge| Orphanet_586|     Cystic fibrosis|[{7_117542108_G_C...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  464|    LYS|   saltbridge| Orphanet_676|Hereditary pancre...| [{7_117548823_G_T}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  465|    THR|        hbond|MONDO_0010178|Congenital bilate...| [{7_117559465_C_A}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  465|    THR|        hbond| Orphanet_586|     Cystic fibrosis| [{7_117559465_C_A}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  465|    THR|metal_complex|MONDO_0010178|Congenital bilate...| [{7_117559465_C_A}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  465|    THR|metal_complex| Orphanet_586|     Cystic fibrosis| [{7_117559465_C_A}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  466|    SER|        hbond|MONDO_0010178|Congenital bilate...|[{7_117559468_C_T...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  466|    SER|        hbond| Orphanet_586|     Cystic fibrosis|[{7_117559468_C_A...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  493|    GLN|        hbond|MONDO_0010178|Congenital bilate...| [{7_117559548_C_T}]|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  493|    GLN|        hbond| Orphanet_586|     Cystic fibrosis|[{7_117559548_C_T...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  650|    PHE|      pistack|  EFO_0000508|Inborn genetic di...|[{7_117592117_C_T...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmi|  650|    PHE|      pistack| Orphanet_586|     Cystic fibrosis|[{7_117592117_C_T...|    [{1xmi}]|\n",
      "|ENSG00000001626|       1xmj|  401|    TRP|      pistack| Orphanet_586|     Cystic fibrosis|[{7_117542101_G_A...|    [{1xmj}]|\n",
      "|ENSG00000001626|       1xmj|  462|    ALA|        hbond| Orphanet_586|     Cystic fibrosis| [{7_117548815_G_A}]|    [{1xmj}]|\n",
      "|ENSG00000001626|       1xmj|  464|    LYS|        hbond| Orphanet_586|     Cystic fibrosis|[{7_117542108_G_C...|    [{1xmj}]|\n",
      "|ENSG00000001626|       1xmj|  464|    LYS|        hbond| Orphanet_676|Hereditary pancre...| [{7_117548823_G_T}]|    [{1xmj}]|\n",
      "+---------------+-----------+-----+-------+-------------+-------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_with_disease_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOLECULES\n",
    "molecule_df = (\n",
    "    spark.read\n",
    "    .parquet(\"../../molecule/\")\n",
    "    .select(\n",
    "        f.col('inchiKey').alias('inchikey'), 'name'\n",
    "        #, 'linkedTargets', 'linkedDiseases'\n",
    "    )\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCHIKEY MOLECULES\n",
    "inchikey_df = (\n",
    "    spark.read\n",
    "        .csv(\"../../inchikey/components_inchikeys.csv\", sep=',', header=True, comment='#')\n",
    "        .select(\n",
    "            f.col('InChIKey').alias('inchikey'), \n",
    "            f.col('CCD_ID').alias('pdbCompound')\n",
    "        )\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOLECULE WITH COMPOUND ID\n",
    "molecules_inchikey_join = (\n",
    "    molecule_df\n",
    "    .join(inchikey_df, on='inchikey')\n",
    "    .drop(\"inchikey\")\n",
    "    .persist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                name|pdbCompound|\n",
      "+--------------------+-----------+\n",
      "|(1-Phenylcyclopen...|        007|\n",
      "|        CHEMBL381806|        008|\n",
      "|      BENZYL ALCOHOL|        010|\n",
      "|           DARUNAVIR|        017|\n",
      "|               N6022|        022|\n",
      "|        CHEMBL243940|        024|\n",
      "|         CHEMBL55264|        028|\n",
      "|         VEMURAFENIB|        032|\n",
      "|       CHEMBL1213083|        039|\n",
      "|             TAK-285|        03P|\n",
      "|          PRINABEREL|        041|\n",
      "|        CHEMBL478524|        047|\n",
      "|         AMINOPTERIN|        04J|\n",
      "|       CHEMBL1229525|        057|\n",
      "|          LASMIDITAN|        05X|\n",
      "|          GSK-256066|        066|\n",
      "|        INFIGRATINIB|        07J|\n",
      "|        CHEMBL305178|        084|\n",
      "|    SULFAMETHOXAZOLE|        08D|\n",
      "|          ALPRAZOLAM|        08H|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "molecules_inchikey_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+---------------+-----------+-----+-----+-------+-------------+---+-------------+-------------+-------------+------------+---------------+-------------+-----------+--------------------+---+---------------+\n",
      "|pdbCompound|        name|         geneId|pdbStructId|resNb|chain|resType|    interType|chr|genLocation_1|genLocation_2|genLocation_3|datasourceId|       targetId|    variantId|  diseaseId|   diseaseFromSource|chr|genomicLocation|\n",
      "+-----------+------------+---------------+-----------+-----+-----+-------+-------------+---+-------------+-------------+-------------+------------+---------------+-------------+-----------+--------------------+---+---------------+\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   17|    R|    THR|        hbond|  7|      6387227|      6387228|      6387229|         eva|ENSG00000136238|7_6387229_G_A|EFO_0009156|Intellectual disa...|  7|        6387229|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   16|    R|    LYS|   saltbridge|  7|      6387224|      6387225|      6387226|         eva|ENSG00000136238|7_6387226_C_G|EFO_0000508|Inborn genetic di...|  7|        6387226|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   17|    R|    THR|metal_complex|  7|      6387227|      6387228|      6387229|         eva|ENSG00000136238|7_6387229_G_A|EFO_0009156|Intellectual disa...|  7|        6387229|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_A|EFO_0000389|Malignant melanom...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387262_C_T|EFO_1001927|Squamous cell car...|  7|        6387262|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_T|EFO_0003859|Malignant neoplas...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387262_C_T|EFO_0000181|Squamous cell car...|  7|        6387262|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_T|EFO_0000756|            Melanoma|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_T|EFO_1001927|Squamous cell car...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_T|EFO_0000389|Malignant melanom...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_A|EFO_1001927|Squamous cell car...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387262_C_T|EFO_0003859|Malignant neoplas...|  7|        6387262|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_T|EFO_0000181|Squamous cell car...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_A|EFO_0000181|Squamous cell car...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387262_C_T|EFO_0000389|Malignant melanom...|  7|        6387262|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   28|    R|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_A|EFO_0003859|Malignant neoplas...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       1g4u|   16|    R|    LYS|        hbond|  7|      6387224|      6387225|      6387226|         eva|ENSG00000136238|7_6387226_C_G|EFO_0000508|Inborn genetic di...|  7|        6387226|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       2h7v|   28|    A|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_A|EFO_0000389|Malignant melanom...|  7|        6387261|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       2h7v|   28|    A|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387262_C_T|EFO_1001927|Squamous cell car...|  7|        6387262|\n",
      "|        GDP|CHEMBL384759|ENSG00000136238|       2h7v|   28|    A|    PHE|      pistack|  7|      6387260|      6387261|      6387262| eva_somatic|ENSG00000136238|7_6387261_C_T|EFO_0003859|Malignant neoplas...|  7|        6387261|\n",
      "+-----------+------------+---------------+-----------+-----+-----+-------+-------------+---+-------------+-------------+-------------+------------+---------------+-------------+-----------+--------------------+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMPOUND NAME\n",
    "disease_ass_comp_name = (\n",
    "    molecules_inchikey_join\n",
    "    .join(res_with_disease, on='pdbCompound')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.>               (8 + 3) / 11]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/marinegirardey/Documents/OpenTargetInternship/scripts/diseases_association.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marinegirardey/Documents/OpenTargetInternship/scripts/diseases_association.ipynb#ch0000016?line=0'>1</a>\u001b[0m disease_ass_comp_name\u001b[39m.\u001b[39;49mshow(\u001b[39m2\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/plip_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=490'>491</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=492'>493</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=493'>494</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=494'>495</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/pyspark/sql/dataframe.py?line=495'>496</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1035'>1036</a>\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1036'>1037</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1037'>1038</a>\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1038'>1039</a>\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/java_gateway.py?line=1039'>1040</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py?line=472'>473</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py?line=473'>474</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py?line=474'>475</a>\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py?line=475'>476</a>\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py?line=476'>477</a>\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/site-packages/py4j/clientserver.py?line=477'>478</a>\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/plip_env/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/socket.py?line=666'>667</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/socket.py?line=667'>668</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/socket.py?line=668'>669</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/socket.py?line=669'>670</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    <a href='file:///Users/marinegirardey/miniforge3/envs/plip_env/lib/python3.8/socket.py?line=670'>671</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "disease_ass_comp_name.show(2, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e967bce37a4bacdfaaf88937c5931e59374b6695986b980bb0c938b9d2bb9028"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('plip_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
